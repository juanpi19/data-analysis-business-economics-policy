{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "**time series data**\n",
    "- Time series data has observations that refer to the same unit but different points in time\n",
    "- The difference between time series observations is the frequency of the time series. It can vary from seconds to years\n",
    "- Time series frequency may also be irregular with gaps in-between. Gaps in time series data can be viewed as missing values of variables\n",
    "- To run a regression of y on x in time series data, the two variables need to be at the same time series frequency\n",
    "- Figuring out the right frequency is essential. Sometimes, aggregation is necessary.\n",
    "- Gaps in time series are frequent. We often ignore them or create binary variables to denote them.\n",
    "- Extreme values are easy to spot on time series graphs. As usual a good solution is to keep them in the data; we may include binary variables to denote them\n",
    "\n",
    "\n",
    "**trend & seasonality**\n",
    "\n",
    "- A variable time series data follows a trend if it tends to change in one direction; in otder words, it has a tendency to increase or decrease.\n",
    "- A time series variable follows a positive trend if its change is positive on average. A change in a variable is also called the first difference\n",
    "- Seasonality means that the value of the variable is expected to follow a cyclical pattern, tracking the seasons off the year, days of the week, or hours of the day\n",
    "- Understanding trends and seasonality is important because they make regression analysis challenging. They are examples of a broader concept, non-stationarity.\n",
    "\n",
    "**stationarity, non-stationarity, random walks**\n",
    "\n",
    "- Stationarity means stability; non-stationarity means the lack of stability\n",
    "- Stationarity time series variables have the same expected value and the same distribution at all times. Trends and seasonality violate stationarity because the expected value is different at different times\n",
    "- Time series variables that follow a random walk change in random, unpredictable ways. Random walks violate another aspect of stability: their spread increases with time.\n",
    "- Random walks are random, completely independent of all the previous steps, no way to use information from the past to predict the next steps.\n",
    "- Phillips-Perron unit root tests:\n",
    "    - null hypothesis: time series data has a unit-root (random walk)\n",
    "    - alternative hypothesis: time series data does not have a unit-root (stationary)\n",
    "\n",
    "\n",
    "**time series regression**\n",
    "- with time series data, we often estimate regressions in changes\n",
    "- α : y is expected to change by α when x doesn’t change.\n",
    "- β : y is expected to change by β more when x increases by one unit more.\n",
    "- We often have variables in relative or percentage changes, or log differences that approximate such relative changes.\n",
    "\n",
    "\n",
    "**trends, seasonality, random walks in a regression**\n",
    "\n",
    "- Consider a simple time series regression in levels and not changes, yEt = α + βxt. If both y and x have a positive trend, the slope coefficient β will be positive whether the two variables are related or not. That is simply because in later time periods both tend to have higher values than in earlier time periods.\n",
    "- associations between variables only because of the effect of trends are said to be spurious, also called spurious regression or spurious correlation.\n",
    "- A good solution to trends is replacing variables in the regression with their changes: first differences\n",
    "- Often, a good solution to seasonality is including binary season variables in the regression; these are called season dummies\n",
    "- Thus, it is good practice to have variables as changes (relative changes, percentage changes, log changes) in time series when we want to estimate average associations or effects. Analyzing changes saves us from spurious regressions because of trends, and it saves us from biased standard error estimates because of random walks.\n",
    "\n",
    "**serial correlation**\n",
    "\n",
    "- serial correlation means correlation of a variable observed at some time with the previous values of the same variable.\n",
    "- Previous observations of a time series variable are called its lagged values\n",
    "- Why should we care about serial correlation in variables? Firt, when a variable is serially correlated, its present value may help predict its value in the near future.\n",
    "- Second, serial correlation creates issues with regressions whatever we use them for. That makes serial correlation an issue\n",
    "\n",
    "\n",
    "**dealing with serial correlation in time series regressions**\n",
    "\n",
    "- serial correlation of variables in time series data does not lead to biased regression coefficients. But it makes the usual standard error estimates wrong.\n",
    "- It is serial correlation in the residual of the regression that is problematic\n",
    "- When we do worry about serial correlation, we have several solutions to get good standard error estimates:\n",
    "    - Newey-West standard errors: incorporates the structure of a serial correlation of the regression residuals into the calculation of the standard error\n",
    "    - lagged dependent variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
